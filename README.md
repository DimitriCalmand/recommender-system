# FinalProject\_2025

## ğŸ“½ï¸ Short Video Recommender System â€“ KuaiRec

This project builds a **personalized, scalable and explainable** shortâ€‘video recommender system using the **KuaiRec dataset**, mirroring the challenges faced by consumerâ€‘grade feeds such as TikTok and Kuaishou.
Our pipeline combines **collaborative filtering, contentâ€‘based retrieval, hybrid ranking and a neural baseline** so that we can simultaneously serve veteran users, firstâ€‘day users and the everâ€‘growing catalogue of new clips.

---

## ğŸ” Objective

Predict, for any given user at any point in time, the *K* short videos that maximise the probability of meaningful engagement (watchâ€‘through, like, share).
On the business side this translates into longer inâ€‘app time, higher retention and more opportunities for monetisation.

---

## ğŸ“¦ Dataset

The **KuaiRec** dataset provides large-scale, fully observed user-video interactions from Kuaishou:

* User-item interactions (views, likes, durations, timestamps)
* Social network information
* Video metadata (tags, categories)
* Daily item features (e.g., popularity, play duration)

---

## ğŸ§¹ Data Processing

All heavyweight I/O lives in `data_processing.ipynb`.

* Robust parsing of nested lists/JSON using **`safe_parse_feat`** (see `utils.py`) â€“ prevents silent schema drift.
* Typeâ€‘safe timestamp conversion (`ensure_datetime`) so that temporal splits and future timeâ€‘aware models remain correct.
* Early *joinâ€‘thenâ€‘save* strategy â€“ we materialise the processed tables once so that every subsequent notebook starts in <3â€¯s on a laptop.

Outputs

```
processed_user_features.csv
processed_video_metadata.csv
processed_interactions_train.csv
processed_interactions_test.csv
```

---

## ğŸ›  Feature Engineering

See `feature_engineering.ipynb`.

| Scope           | Key features                                                   | Why we kept them                                                       |
| --------------- | -------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **User**        | `total_video_watched`, `avg_watch_ratio`, `preferred_category` | Capture longâ€‘term taste while remaining stationary over short windows  |
| **Video**       | `total_likes`, `avg_play_duration`, hashed `video_tags`        | Provide coldâ€‘start signal and normalise popularity across upload dates |
| **Interaction** | `user_video_watch_count`, `user_video_avg_watch_ratio`         | Encode familiarity and freshness of a clip for a user                  |
| **Social**      | `num_friends`, `friends_favorite_categories`                   | Exploit homophily observed in KuaiRecâ€™s social graph                   |

---

## âš™ï¸ Design RationaleÂ â€” How We Chose the Final Stack

| Challenge                                              | Decision                                          | Rationale                                                                                                                            |
| ------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| Implicit, heavyâ€‘tailed feedback                        | **ALS with confidence weighting**                 | Handles 10â¶ users Ã— 10â¶ items, optimised C++ backend, supports nonâ€‘binary â€œstrengthâ€ via watchâ€‘ratio                                 |
| Coldâ€‘start & catalogue refresh (\~30â€¯k new videos/day) | **TFâ€‘IDFâ€¯+â€¯cosine content model**                 | Requires only metadata; zeroâ€‘shot for unseen clips; trivially parallelisable                                                         |
| Popularity bias & temporal drift                       | **Hybrid ranker (ALSâ€¯Ã— popularity blend Î±=0.15)** | Preserves personalisation while guaranteeing exposure of trending content                                                            |
| Prototype deep learning                                | **Embeddingâ€‘FC RecNN**                            | Baseline to validate oneâ€‘hot â†’ dense embeddings pipeline; easily extensible to sequence/transformer models when compute budget grows |
| Diversity & user fatigue                               | **Categoryâ€‘aware metrics + reranking**            | Explicit objectives (`CategoryPrecision`, `CategoryHitRate`) proved to correlate with subjective diversity in manual spotâ€‘checks     |

---

## ğŸ§  Models & Methods

1. **Collaborative Filtering â€“ Alternating Least Squares (implicit)**
   *Trains on a sparse userâ€¯Ã—â€¯videoâ€¯Ã—â€¯watchâ€‘ratio matrix (â‰ˆ1.2â€¯B nonâ€‘zeros).*

   * Pros: linear in interactions, GPUâ€‘optional, strong for power users.
   * Cons: coldâ€‘start blind.
   * **Why chosen:** Only algorithm that finished gridâ€‘search (ranksâ€¯=â€¯64, regâ€¯=â€¯1eâ€‘4) under 2â€¯hours on an M4 Mac while delivering >0.80 NDCG\@30.

2. **Contentâ€‘Based Retrieval â€“ TFâ€‘IDF Cosine**
   *Builds binary tag and hierarchical category vectors.*

   * Pros: instantly covers new uploads; explanations are humanâ€‘readable.
   * **Why chosen:** KuaiRec tags are curated; experiments with Doc2Vec gave negligible gains at 20Ã— cost.

   3. **Popularityâ€‘Aware Hybrid**

      * **Why chosen:** Small Î± improves *HitRate* for new and sparse users without hurting precision for veterans.

4. **RecNN â€“ Feedâ€‘Forward Embedding Network** (`RecNN.py`)
   *448â€‘d concatenated embeddings â†’ 128 â†’ 64 â†’ 32 â†’ sigmoid.*

   * **Why chosen:** Serves as a controllable deep baseline and a code path for future sequence models; embedding tables reveal cardinality issues early.

---

## ğŸ“Š Evaluation

The neural network and als model have their own metrics in the model_training notebook.

**Neural network metrics:**
- `Precision: 0.47`
- `F1: 0.64`

**ALS metrics:**
- `NDCG@30 = 0.805` - Best for position-aware ranking quality.
- `MAP@30 = 0.687` â€“ Great global ranking evaluation.
- `Precision@30 = 0.789` - Simple and intuitive.

### Recommendations evaluation
> âš ï¸ **Important** âš ï¸
> 
> This function evaluates how well the recommended videos match what users actually watched or what theyâ€™re likely interested in (based on categories).
>
> But these results donâ€™t tell us if the other recommended videos are bad â€” a low precision just means the user didnâ€™t watch them, not that they wouldnâ€™t like them. Similarly, matching a category means it's relevant, but not necessarily that the user would enjoy it.
> 
> So overall, these metrics help comparing the models but don't say a lot about recommendations quality. This is why the model metrics above are important for measuring the recommendation quality.

Performed in `evaluation.ipynb`. Metrics used:

* **Precision\@K** â€“ proportion of relevant items in top-K
* **HitRate\@K** â€“ how often at least one recommended item is relevant
* **CategoryPrecision\@K / CategoryHitRate\@K** â€“ diversity and relevance in content categories

| Method        | Precision\@10 | CategoryPrecision | HitRate\@10 | CategoryHitRate |
| ------------- | ------------- | ----------------- | ----------- | --------------- |
| ALS           | 0.029         | 0.014             | 0.87        | 0.422           |
| Content-Based | 0.020         | 0.302             | 0.434       | 0.907           |
| Hybrid        | 0.039         | 0.333             | 0.689       | 1.000           |
| Neural Net    | 0.004         | 0.064             | 0.124       | 0.244           |
| **Combined**  | **0.049**     | **0.318**         | **0.911**   | **0.943**       |

**Combined approach** got the best overall performance, combining personalization, diversity, and coverage.

---

## ğŸ¤” Why Multiple Approaches?

Using multiple models improves robustness and flexibility:

* **Collaborative Filtering**: excels in personalizing for users with rich histories.
* **Content-Based Filtering**: supports cold start and new video discovery.
* **Hybridization**: leverages popularity to address sparsity and boost relevance.
* **Neural Networks**: provide a flexible foundation for future deep learning improvements (e.g., attention, sequence modeling).

Together, they cover a broad range of user scenarios and complement each otherâ€™s weaknesses.

---

## ğŸ“ File Execution Order

Run the notebooks in the following order:

1. `data_processing.ipynb`
2. `feature_engineering.ipynb`
3. `model_training.ipynb`
4. `recommendation.ipynb`
5. `evaluation.ipynb`

---

## âœ… Conclusion

This project demonstrates a complete pipeline for building a short video recommender system, from data preprocessing to hybrid modeling and evaluation. Future work could improve neural performance, incorporate time-based or sequence-aware models, and experiment with advanced ranking losses.