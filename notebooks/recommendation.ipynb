{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:10.353424Z",
     "start_time": "2025-05-16T10:07:08.840841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from src.utils.RecNN import RecNN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import pickle"
   ],
   "id": "25c7be82099c500e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the models and datasets",
   "id": "5617befa47e1f415"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:28.365754Z",
     "start_time": "2025-05-16T10:07:10.356622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K = 10  # Number of recommendations to generate\n",
    "\n",
    "processed_path = \"../data/processed/\"\n",
    "\n",
    "# Load the processed datasets\n",
    "interactions_train = pd.read_csv(os.path.join(processed_path, \"interactions_train.csv\"))\n",
    "interactions_test = pd.read_csv(os.path.join(processed_path, \"interactions_test.csv\"))\n",
    "user_features = pd.read_csv(os.path.join(processed_path, \"user_features.csv\"))\n",
    "video_metadata = pd.read_csv(os.path.join(processed_path, \"video_metadata.csv\"))\n",
    "\n",
    "# Load the models\n",
    "model_path = \"../models/\"\n",
    "\n",
    "# Neural Network Model\n",
    "model_nn_state_dict = torch.load(os.path.join(model_path, \"nn_model.pth\"), map_location=torch.device('cpu'))\n",
    "model_nn = RecNN()\n",
    "model_nn.load_state_dict(model_nn_state_dict)\n",
    "\n",
    "# ALS Model\n",
    "with open(os.path.join(model_path, 'als_model.pkl'), 'rb') as f:\n",
    "    model_als = pickle.load(f)\n",
    "\n",
    "# Similarity Matrix for Content-Based Filtering\n",
    "similarity_matrix = np.load(os.path.join(model_path, 'similarity_matrix.npy'))"
   ],
   "id": "117750159cb4e2c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jules_/Desktop/Epita/Algo/recommender_system_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recommendations ALS Model",
   "id": "c1146c2c09bd251"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:30.023689Z",
     "start_time": "2025-05-16T10:07:28.553232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "interaction_matrix = csr_matrix((interactions_train['watch_ratio'],\n",
    "                                 (interactions_train['user_id'], interactions_train['video_id'])))\n",
    "\n",
    "def recommend_als(model, user_ids, video_ids, top_n=5):\n",
    "    recommendations_als = {}\n",
    "    for user_id in user_ids:\n",
    "        # Get the user's recommendations from the ALS model\n",
    "        if 0 <= user_id < interaction_matrix.shape[0]:\n",
    "            user_recs = model.recommend(user_id, interaction_matrix[user_id], N=top_n, filter_already_liked_items=True)\n",
    "            # Extract the video IDs from the recommendations\n",
    "            rec_video_ids = [rec[0] for rec in user_recs]\n",
    "            recommendations_als[user_id] = rec_video_ids\n",
    "    return recommendations_als\n",
    "\n",
    "train_video_ids = set(interactions_train['video_id'].unique())\n",
    "valid_video_ids = [vid for vid in video_metadata['video_id'].unique() if vid in train_video_ids]\n",
    "\n",
    "user_ids = user_features['user_id'].values\n",
    "\n",
    "als_recommendations = recommend_als(model_als, user_ids, valid_video_ids, top_n=K)"
   ],
   "id": "de3ce37c62e04fa3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recommendations Content-Based",
   "id": "843c10ca6abeccd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:08:09.259472Z",
     "start_time": "2025-05-16T10:07:30.031813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recommend_cb(sim_matrix, video_ids, top_n=10):\n",
    "    recommendations = {}\n",
    "    for user_idx, sims in enumerate(sim_matrix):\n",
    "        top_indices = np.argsort(sims)[::-1]\n",
    "        unique_recs = []\n",
    "        seen = set()\n",
    "        for idx in top_indices:\n",
    "            vid = video_ids[idx]\n",
    "            if vid not in seen:\n",
    "                unique_recs.append(vid)\n",
    "                seen.add(vid)\n",
    "            if len(unique_recs) == top_n:\n",
    "                break\n",
    "        recommendations[user_ids[user_idx - 1]] = unique_recs\n",
    "    return recommendations\n",
    "\n",
    "video_ids = video_metadata['video_id'].values\n",
    "\n",
    "cb_recommendations = recommend_cb(similarity_matrix, video_ids, K)"
   ],
   "id": "eb75eb59d8c15985",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recommendations Hybrid",
   "id": "aa83bded9bd4ca72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:09:57.079962Z",
     "start_time": "2025-05-16T10:08:09.322213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize popularity\n",
    "video_metadata['normalized_popularity'] = (\n",
    "    (video_metadata['like_cnt'] - video_metadata['like_cnt'].min()) /\n",
    "    (video_metadata['like_cnt'].max() - video_metadata['like_cnt'].min())\n",
    ")\n",
    "popularity_scores = video_metadata.set_index('video_id')['normalized_popularity'].reindex(video_ids).fillna(0).values\n",
    "\n",
    "def recommend_hybrid(sim_matrix, video_ids, popularity_scores, alpha=0.7, top_n=10):\n",
    "    recommendations = {}\n",
    "    for user_idx, sims in enumerate(sim_matrix):\n",
    "        # Blend content similarity with popularity\n",
    "        final_scores = alpha * sims + (1 - alpha) * popularity_scores\n",
    "\n",
    "        top_indices = np.argsort(final_scores)[::-1]\n",
    "        unique_recs = []\n",
    "        seen = set()\n",
    "        for idx in top_indices:\n",
    "            vid = video_ids[idx]\n",
    "            if vid not in seen:\n",
    "                unique_recs.append(vid)\n",
    "                seen.add(vid)\n",
    "            if len(unique_recs) == top_n:\n",
    "                break\n",
    "        recommendations[user_ids[user_idx]] = unique_recs\n",
    "    return recommendations\n",
    "\n",
    "hybrid_recommendations = recommend_hybrid(similarity_matrix, video_ids, popularity_scores, alpha=0.7, top_n=K)"
   ],
   "id": "a042dfe58c324b10",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recommendations Neural Network",
   "id": "2ea351cdb7b35814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:13:55.076827Z",
     "start_time": "2025-05-16T10:09:57.144726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recommend_nn(model, user_features_df, video_metadata_df, top_n=10):\n",
    "    model.eval()\n",
    "    recommendations = {}\n",
    "    video_ids = video_metadata_df['video_id'].values\n",
    "    video_tags = video_metadata_df['video_tag_id'].infer_objects(copy=False).fillna(0).astype(int).values\n",
    "\n",
    "    # Prepare user features\n",
    "    begin = datetime.now()\n",
    "    for _, user_row in user_features_df.iterrows():\n",
    "        user_id = user_row['user_id']\n",
    "        if user_id % 100 == 0:\n",
    "            print(f\"{user_id}/{user_features_df.shape[0]} users processed in {datetime.now() - begin}\")\n",
    "        onehot_feats = [f'onehot_feat{i}' for i in range(1, 18)]\n",
    "        user_input = user_row[onehot_feats].infer_objects(copy=False).fillna(0).to_numpy(dtype=np.int64)\n",
    "\n",
    "        inputs = []\n",
    "        for tag in video_tags:\n",
    "            x = np.concatenate([user_input, [tag]])\n",
    "            inputs.append(x)\n",
    "\n",
    "        inputs_tensor = torch.tensor(np.array(inputs), dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            scores = model(inputs_tensor).squeeze().numpy()\n",
    "\n",
    "        top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "        recommended_videos = video_ids[top_indices]\n",
    "        recommendations[user_id] = recommended_videos.tolist()\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "nn_recommendations = recommend_nn(model_nn, user_features[:500], video_metadata, top_n=K) # TODO: remove limit"
   ],
   "id": "5333858bf19665bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/500 users processed in 0:00:00.000787\n",
      "100/500 users processed in 0:00:47.844554\n",
      "200/500 users processed in 0:01:35.296783\n",
      "300/500 users processed in 0:02:23.376665\n",
      "400/500 users processed in 0:03:10.683394\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Combine Recommendations",
   "id": "117bb191bc641f36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:13:55.207008Z",
     "start_time": "2025-05-16T10:13:55.115610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_three_recommendations(rec1, rec2, rec3, alpha=0.5, beta=0.3):\n",
    "    combined_recs = {}\n",
    "    gamma = 1 - alpha - beta\n",
    "\n",
    "    for user_id in set(rec1.keys()).union(rec2.keys()).union(rec3.keys()):\n",
    "        rec1_list = rec1.get(user_id, [])\n",
    "        rec2_list = rec2.get(user_id, [])\n",
    "        rec3_list = rec3.get(user_id, [])\n",
    "\n",
    "        # Rank scores: higher rank = higher number\n",
    "        rec1_scores = {vid: len(rec1_list) - idx for idx, vid in enumerate(rec1_list)}\n",
    "        rec2_scores = {vid: len(rec2_list) - idx for idx, vid in enumerate(rec2_list)}\n",
    "        rec3_scores = {vid: len(rec3_list) - idx for idx, vid in enumerate(rec3_list)}\n",
    "\n",
    "        # Count how many lists each video appears in\n",
    "        video_counts = defaultdict(int)\n",
    "        for vid in set(rec1_list):\n",
    "            video_counts[vid] += 1\n",
    "        for vid in set(rec2_list):\n",
    "            video_counts[vid] += 1\n",
    "        for vid in set(rec3_list):\n",
    "            video_counts[vid] += 1\n",
    "\n",
    "        # Union of all videos\n",
    "        all_vids = set(rec1_list) | set(rec2_list) | set(rec3_list)\n",
    "\n",
    "        combined_scores = {}\n",
    "        for vid in all_vids:\n",
    "            score1 = rec1_scores.get(vid, 0)\n",
    "            score2 = rec2_scores.get(vid, 0)\n",
    "            score3 = rec3_scores.get(vid, 0)\n",
    "            combined_scores[vid] = alpha * score1 + beta * score2 + gamma * score3\n",
    "\n",
    "        # Sort videos by score\n",
    "        sorted_vids = sorted(combined_scores.keys(), key=lambda x: combined_scores[x], reverse=True)\n",
    "\n",
    "        # Guarantee: add any video that appears in at least 2 lists if not already in sorted list\n",
    "        must_include = {vid for vid, count in video_counts.items() if count >= 2}\n",
    "        final_list = []\n",
    "        seen = set()\n",
    "\n",
    "        for vid in sorted_vids:\n",
    "            final_list.append(vid)\n",
    "            seen.add(vid)\n",
    "\n",
    "        for vid in must_include:\n",
    "            if vid not in seen:\n",
    "                final_list.append(vid)\n",
    "\n",
    "        combined_recs[user_id] = final_list\n",
    "\n",
    "    return combined_recs\n",
    "\n",
    "# Combine content-based and ALS recommendations\n",
    "combined_recommendations = combine_three_recommendations(cb_recommendations, als_recommendations, nn_recommendations, alpha=0.5, beta=0.3)"
   ],
   "id": "569b2f47af4d0ca",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Recommendations",
   "id": "e78f7b5f7708a12d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:26:36.670441Z",
     "start_time": "2025-05-16T10:26:36.503070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save to CSV\n",
    "\n",
    "def save_recommendations(recommendations, filename):\n",
    "    # Create a DataFrame from the recommendations dictionary\n",
    "    df = pd.DataFrame.from_dict(recommendations, orient='index')\n",
    "    # Reset index to make user_id a column\n",
    "    df = df.reset_index()\n",
    "    # Rename the index column to user_id\n",
    "    df = df.rename(columns={'index': 'user_id'})\n",
    "    # Save to CSV\n",
    "    df.to_csv(os.path.join(\"../data/recommendations/\", filename), index=False)\n",
    "\n",
    "save_recommendations(als_recommendations, \"als_recommendations.csv\")\n",
    "\n",
    "save_recommendations(cb_recommendations, \"content_based_recommendations.csv\")\n",
    "\n",
    "save_recommendations(hybrid_recommendations, \"hybrid_recommendations.csv\")\n",
    "\n",
    "save_recommendations(nn_recommendations, \"nn_recommendations.csv\")\n",
    "\n",
    "save_recommendations(combined_recommendations, \"combined_recommendations.csv\")"
   ],
   "id": "a88666a8958733cc",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
